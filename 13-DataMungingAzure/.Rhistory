setwd("C:/Users/paloma/Desktop/GitHub/R-Tools/13-DataMungingAzure")
setwd("C:/Users/paloma/Desktop/GitHub/R-Tools/13-DataMungingAzure")
getwd()
install.packages("mlbench")
library(mlbench)
data("HouseVotes84")
View(HouseVotes84)
plot(as.factor(HouseVotes84[,2]))
title(main = "Votes cast for issue", xlab = "vote", ylab = "# reps")
plot(as.factor(HouseVotes84[HouseVotes84$Class == 'republican', 2]))
title(main = "Republican votes cast for issue 1", xlab = "vote", ylab = "# reps")
plot(as.factor(HouseVotes84[HouseVotes84$Class == 'democrat',2]))
title(main = "Democrat votes cast for issue 1", xlab = "vote", ylab = "# reps")
na_by_col_class <- function (col,cls){return(sum(is.na(HouseVotes84[,col]) & HouseVotes84$Class==cls))}
p_y_col_class <- function(col,cls){
sum_y <- sum(HouseVotes84[,col] == 'y' & HouseVotes84$Class == cls, na.rm = TRUE)
sum_n <- sum(HouseVotes84[,col] == 'n' & HouseVotes84$Class == cls, na.rm = TRUE)
return(sum_y/(sum_y+sum_n))}
p_y_col_class(2,'democrat')
p_y_col_class(2,'republican')
na_by_col_class(2,'democrat')
na_by_col_class(2,'republican')
for (i in 2:ncol(HouseVotes84)) {
if(sum(is.na(HouseVotes84[,i])>0)) {
c1 <- which(is.na(HouseVotes84[,i]) & HouseVotes84$Class == 'democrat',arr.ind = TRUE)
c2 <- which(is.na(HouseVotes84[,i]) & HouseVotes84$Class == 'republican',arr.ind = TRUE)
HouseVotes84[c1,i] <- ifelse(runif(na_by_col_class(i,'democrat'))<p_y_col_class(i,'democrat'),'y','n')
HouseVotes84[c2,i] <- ifelse(runif(na_by_col_class(i,'republican'))<p_y_col_class(i,'republican'),'y','n')}
}
HouseVotes84[,"train"] <- ifelse(runif(nrow(HouseVotes84)) < 0.80,1,0)
trainColNum <- grep("train",names(HouseVotes84))
trainHouseVotes84 <- HouseVotes84[HouseVotes84$train == 1, -trainColNum]
testHouseVotes84 <- HouseVotes84[HouseVotes84$train == 0, -trainColNum]
install.packages("e1071")
library(e1071)
nb_model <- naiveBayes(Class ~ ., data = trainHouseVotes84)
summary(nb_model)
nb_model
str(nb_model)
nb_test_predict <- predict(nb_model, testHouseVotes84[,-1])
table(pred = nb_test_predict, true = testHouseVotes84$Class)
mean(nb_test_predict == testHouseVotes84$Class)
nb_multiple_runs <- function(train_fraction, n) {
fraction_correct <- rep(NA,n)
for (i in 1:n) {
HouseVotes84[,"train"] <- ifelse(runif(nrow(HouseVotes84))<train_fraction,1,0)
trainColNum <- grep("train", names(HouseVotes84))
trainHouseVotes84 <- HouseVotes84[HouseVotes84$train == 1,-trainColNum]
testHouseVotes84 <- HouseVotes84[HouseVotes84$train == 0,-trainColNum]
nb_model <- naiveBayes(Class ~ ., data = trainHouseVotes84)
nb_test_predict <- predict(nb_model, testHouseVotes84[,-1])
fraction_correct[i] <- mean(nb_test_predict == testHouseVotes84$Class)
}
return(fraction_correct)
}
fraction_correct_predictions <- nb_multiple_runs(0.8, 20)
fraction_correct_predictions
summary(fraction_correct_predictions)
sd(fraction_correct_predictions)
getwd()
dados_treino <- read.csv('titanic-train.csv')
setwd("C:/Users/paloma/Desktop/GitHub/R-Tools/13-DataMungingAzure")
getwd()
View(dados_treino)
install.packages("Amelia")
library(Amelia)
missmap(dados_treino,
main = "Titanic Training Data - Mapa de Dados Missing",
col = c("yellow", "black"),
legend = FALSE)
library(ggplot2)
ggplot(dados_treino,aes(Survived)) + geom_bar()
ggplot(dados_treino,aes(Pclass)) + geom_bar(aes(fill = factor(Pclass)), alpha = 0.5)
ggplot(dados_treino,aes(Sex)) + geom_bar(aes(fill = factor(Sex)), alpha = 0.5)
ggplot(dados_treino,aes(Age)) + geom_histogram(fill = 'blue', bins = 20, alpha = 0.5)
ggplot(dados_treino,aes(SibSp)) + geom_bar(fill = 'red', alpha = 0.5)
ggplot(dados_treino,aes(Fare)) + geom_histogram(fill = 'green', color = 'black', alpha = 0.5)
pl <- ggplot(dados_treino, aes(Pclass,Age)) + geom_boxplot(aes(group = Pclass, fill = factor(Pclass), alpha = 0.4))
pl + scale_y_continuous(breaks = seq(min(0), max(80), by = 2))
impute_age <- function(age, class){
out <- age
for (i in 1:length(age)){
if (is.na(age[i])){
if (class[i] == 1){
out[i] <- 37
}else if (class[i] == 2){
out[i] <- 29
}else{
out[i] <- 24
}
}else{
out[i]<-age[i]
}
}
return(out)
}
fixed.ages <- impute_age(dados_treino$Age, dados_treino$Pclass)
dados_treino$Age <- fixed.ages
missmap(dados_treino,
main = "Titanic Training Data - Mapa de Dados Missing",
col = c("yellow", "black"),
legend = FALSE)
str(dados_treino)
head(dados_treino, 3)
library(dplyr)
install.packages("dplyr")
library(dplyr)
dados_treino <- select(dados_treino, -PassengerId, -Name, -Ticket, -Cabin)
head(dados_treino, 3)
str(dados_treino)
log.model <- glm(formula = Survived ~ . , family = binomial(link = 'logit'), data = dados_treino)
summary(log.model)
summary(log.model)
library(caTools)
install.packages("caTools")
library(caTools)
set.seed(101)
split = sample.split(dados_treino$Survived, SplitRatio = 0.70)
dados_treino_final = subset(dados_treino, split == TRUE)
dados_teste_final = subset(dados_treino, split == FALSE)
final.log.model <- glm(formula = Survived ~ . , family = binomial(link='logit'), data = dados_treino_final)
summary(final.log.model)
fitted.probabilities <- predict(final.log.model, newdata = dados_teste_final, type = 'response')
fitted.results <- ifelse(fitted.probabilities > 0.5, 1, 0)
misClasificError <- mean(fitted.results != dados_teste_final$Survived)
print(paste('Acuracia', 1-misClasificError))
table(dados_teste_final$Survived, fitted.probabilities > 0.5)
