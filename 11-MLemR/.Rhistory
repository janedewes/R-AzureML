setwd("C:/Users/paloma/Desktop/GitHub/R-Tools/11-MLemR")
setwd("C:/Users/paloma/Desktop/GitHub/R-Tools/11-MLemR")
getwd()
x <- seq(0, 100)
y <- 2 * x + 35
x
y
y1 <- y + rnorm(101, 0, 50)
y1
hist(y1)
plot(x, y1, pch = 19,  xlab = 'X', ylab = 'Y')
modelo <- lm(y1 ~ x)
modelo
modelo
class(modelo)
a <- modelo$coefficients[1]
b <- modelo$coefficients[2]
y2 <- a + b*x
lines(x, y2, lwd = 2)
y3 <- (y2[51]-50*(b-1))+(b-1)*x
y4 <- (y2[51]-50*(b+1))+(b+1)*x
y5 <- (y2[51]-50*(b+2))+(b+2)*x
lines(x,y3,lty=3)
lines(x,y4,lty=3)
lines(x,y5,lty=3)
Idade <- c(9,13,14,21,15,18,20,8,14,23,16,21,10,12,20,
9,13,5,15,21)
Tempo <- c(17.87,13.75,12.72,6.98,11.01,10.48,10.19,19.11,
12.72,0.45,10.67,1.59,14.91,14.14,9.40,16.23,
12.74,20.64,12.34,6.44)
plot(Idade, Tempo,
xlab = 'Idade',
ylab = 'Tempo de Reação')
modelo <- lm(Tempo ~ Idade)
modelo
reta <- 25.8134 - 0.9491 *Idade
# Crie o gráfico da reta
lines(Idade, reta)
alturas = c(176, 154, 138, 196, 132, 176, 181, 169, 150, 175)
pesos = c(82, 49, 53, 112, 47, 69, 77, 71, 62, 78)
plot(alturas, pesos, pch = 16, cex = 1.3, col = "blue",
main = "Altura x Peso",
ylab = "Peso Corporal (kg)",
xlab = "Altura (cm)")
modelo <- lm(pesos ~ alturas)
modelo
abline( -70.4627, 0.8528)
summary(modelo)
alturas2 = data.frame(c(179, 152, 134, 197, 131, 178, 185, 162, 155, 172))
previsao <- predict(modelo, alturas2)
previsao
plot(alturas, pesos, pch = 16, cex = 1.3,
col = "blue",
main = "Altura x Peso",
ylab = "Peso (kg)",
xlab = "Altura (cm)")
abline(lm(pesos ~ alturas))
# Obtendo o tamanho de uma das amostras de dados
num <- length(alturas)
num
for (k in 1: num)
lines(c(alturas[k], alturas[k]),
c(pesos[k], pesos[k]))
par(mfrow = c(2,2)
plot(modelo)
par(mfrow = c(2,2))
plot(modelo)
getwd()
setwd("C:/Users/paloma/Desktop/GitHub/R-Tools/11-MLemR")
getwd()
df <- read.csv2('estudantes.csv')
View(df)
head(df)
summary(df)
str(df)
any(is.na(df))
library(ggplot2)
library(ggthemes)
library(dplyr)
View(df)
colunas_numericas <- sapply(df, is.numeric)
colunas_numericas
data_cor <- cor(df[,colunas_numericas])
data_cor
head(data_cor)
library(corrplot)
library(corrgram)
install.packages("corrgram")
install.packages('corrplot')
corrplot(data_cor, method = 'color')
corrgram(df)
corrgram(df, order = TRUE, lower.panel = panel.shade,
upper.panel = panel.pie, text.panel = panel.txt)
corrgram(df)
library(corrgram)
corrgram(df)
corrgram(df, order = TRUE, lower.panel = panel.shade,
upper.panel = panel.pie, text.panel = panel.txt)
ggplot(df, aes(x = G3)) +
geom_histogram(bins = 20,
alpha = 0.5, fill = 'blue') +
theme_minimal()
install.packages("caTools")
library(caTools)
set.seed(101)
amostra <- sample.split(df$age, SplitRatio = 0.70)
treino = subset(df, amostra == TRUE)
teste = subset(df, amostra == FALSE)
modelo_v1 <- lm(G3 ~ ., treino)
modelo_v2 <- lm(G3 ~ G2 + G1, treino)
modelo_v3 <- lm(G3 ~ absences, treino)
modelo_v4 <- lm(G3 ~ Medu, treino)
summary(modelo_v1)
summary(modelo_v2)
summary(modelo_v3)
summary(modelo_v4)
res <- residuals(modelo_v1)
res <- as.data.frame(res)
head(res)
ggplot(res, aes(res)) +
geom_histogram(fill = 'blue',
alpha = 0.5,
binwidth = 1)
plot(modelo_v1)
modelo_v1 <- lm(G3 ~ ., treino)
prevendo_G3 <- predict(modelo_v1, teste)
prevendo_G3
resultados <- cbind(prevendo_G3, teste$G3)
colnames(resultados) <- c('Previsto','Real')
resultados <- as.data.frame(resultados)
resultados
min(resultados)
trata_zero <- function(x){
if  (x < 0){
return(0)
}else{
return(x)
}
}
resultados$Previsto <- sapply(resultados$Previsto, trata_zero)
resultados$Previsto
mse <- mean((resultados$Real - resultados$Previsto)^2)
print(mse)
SSE = sum((resultados$Previsto - resultados$Real)^2)
SST = sum((mean(df$G3) - resultados$Real)^2)
R2 = 1 - (SSE/SST)
R2
getwd()
library(MASS)
# Importando os dados do dataset Boston
set.seed(101)
dados <- Boston
head(dados)
View(dados)
str(dados)
summary(dados)
any(is.na(dados))
install.packages("neuralnet")
library(neuralnet)
maxs <- apply(dados, 2, max)
mins <- apply(dados, 2, min)
maxs
mins
dados_normalizados <- as.data.frame(scale(dados, center = mins, scale = maxs - mins))
head(dados_normalizados)
install.packages("caTools")
library(caTools)
split = sample.split(dados_normalizados$medv, SplitRatio = 0.70)
treino = subset(dados_normalizados, split == TRUE)
teste = subset(dados_normalizados, split == FALSE)
coluna_nomes <- names(treino)
coluna_nomes
formula <- as.formula(paste("medv ~", paste(coluna_nomes[!coluna_nomes %in% "medv"], collapse = " + ")))
formula
rede_neural <- neuralnet(formula, data = treino, hidden = c(5,3), linear.output = TRUE)
plot(rede_neural)
rede_neural_prev <- compute(rede_neural, teste[1:13])
rede_neural_prev
str(rede_neural_prev)
previsoes <- rede_neural_prev$net.result * (max(dados$medv) - min(dados$medv)) + min(dados$medv)
teste_convert <- (teste$medv) * (max(dados$medv) - min(dados$medv)) + min(dados$medv)
teste_convert
MSE.nn <- sum((teste_convert - previsoes)^2)/nrow(teste)
MSE.nn
error.df <- data.frame(teste_convert, previsoes)
head(error.df)
# Plot dos erros
library(ggplot2)
ggplot(error.df, aes(x = teste_convert,y = previsoes)) +
geom_point() + stat_smooth()
getwd()
setwd("C:/Users/paloma/Desktop/GitHub/R-Tools/11-MLemR")
getwd()
letters <- read.csv("letterdata.csv")
str(letters)
View(letters)
letters_treino <- letters[1:16000, ]
letters_teste  <- letters[16001:20000, ]
install.packages("kernlab")
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letters_treino, kernel = "vanilladot")
letter_classifier <- ksvm(letter ~ ., data = letters_treino, kernel = "vanilladot") # var target -> letter
letters <- read.csv("letterdata.csv")
letters_treino <- letters[1:16000, ]
letters_teste  <- letters[16001:20000, ]
install.packages("kernlab")
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letters_treino, kernel = "vanilladot")
letter_classifier <- ksvm(letter ~ ., data = letters_treino, kernel = "vanilladot")
letter_classifier <- ksvm(letter ~ ., data = letters_treino, kernel = "vanilladot")
View(letters)
letters_treino <- letters[1:16000, ]
letters_teste  <- letters[16001:20000, ]
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letters_treino, kernel = "vanilladot")
# Visualizando resultado do modelo
letter_classifier
letters <- read.csv("letterdata.csv")
letters <- read.csv("letterdata.csv")
letter_classifier <- ksvm(letter ~ ., data = letters_treino, kernel = "vanilladot")
letter_classifier <- ksvm(letter ~ , data = letters_treino, kernel = "vanilladot")
letter_classifier <- ksvm(letter ~. , data = letters_treino, kernel = "vanilladot")
letter_classifier <- ksvm(letter ~ ., data = letters_treino, kernel = "vanilladot")
letter_classifier <- ksvm(letter, ~ ., data = letters_treino, kernel = "vanilladot")
letter_classifier <- ksvm(letter ~ ., data = letters_treino, kernel = "vanilladot")
